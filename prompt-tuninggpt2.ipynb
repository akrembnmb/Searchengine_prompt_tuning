{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bert-score ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score ","metadata":{"execution":{"iopub.status.idle":"2024-06-11T12:53:48.320826Z","shell.execute_reply.started":"2024-06-11T12:53:33.446986Z","shell.execute_reply":"2024-06-11T12:53:48.319701Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Installing collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport csv\nimport nltk\nimport random\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nimport torch\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn as nn\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Model, GPT2Config, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nfrom bert_score import BERTScorer\nfrom transformers import BertTokenizer, BertForMaskedLM, BertModel","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:17.364795Z","iopub.execute_input":"2024-06-11T12:55:17.365278Z","iopub.status.idle":"2024-06-11T12:55:17.464900Z","shell.execute_reply.started":"2024-06-11T12:55:17.365217Z","shell.execute_reply":"2024-06-11T12:55:17.464148Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def squad_json_to_dataframe(file_path, record_path=['data','paragraphs','qas','answers']):\n\n    file = json.loads(open(file_path).read())\n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file,record_path[:-2])\n    # combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    m['context'] = idx\n    data = m[['id','question','context','answers']].set_index('id').reset_index()\n    data['c_id'] = data['context'].factorize()[0]\n    return data\n\ndef preprocess_text_en(data):\n    # Convert to lowercase\n    data2 = []\n    for text in data:\n        # Skip None values\n        if text is None:\n            continue\n        text = text.lower()\n\n        # Tokenization\n        tokens = word_tokenize(text)\n\n        # Remove stop words\n        stop_words = set(stopwords.words('english'))\n        tokens = [word for word in tokens if word not in stop_words]\n\n        # Join the tokens back into a string\n        preprocessed_text = ' '.join(tokens)\n        data2.append(preprocessed_text)\n\n    return data2\n    \n\ndef calculate_bleu_score(machine_results, reference_texts):\n    bleu_score = corpus_bleu([[ref.split()] for ref in reference_texts], [gen.split() for gen in machine_results])\n    return bleu_score\n\ndef calculate_rouge_scores(generated_answers, ground_truth):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    total_rouge1, total_rouge2, total_rougeL = 0, 0, 0\n    for gen, ref in zip(generated_answers, ground_truth):\n        scores = scorer.score(gen, ref)\n        total_rouge1 += scores['rouge1'].fmeasure\n        total_rouge2 += scores['rouge2'].fmeasure\n        total_rougeL += scores['rougeL'].fmeasure\n    average_rouge1 = total_rouge1 / len(generated_answers)\n    average_rouge2 = total_rouge2 / len(generated_answers)\n    average_rougeL = total_rougeL / len(generated_answers)\n    return average_rouge1, average_rouge2, average_rougeL\n\ndef calculate_bert_score(generated_answers, ground_truth):\n    scorer = BERTScorer(model_type='bert-base-uncased')\n    P, R, F1 = scorer.score(generated_answers, ground_truth)\n    avg_precision = sum(p.mean() for p in P) / len(P)\n    avg_recall = sum(r.mean() for r in R) / len(R)\n    avg_f1 = sum(f1.mean() for f1 in F1) / len(F1)\n    return avg_precision, avg_recall, avg_f1\n\n\n# Define a function to tokenize, convert text to indices, and pad sequences\ndef tokenize_and_pad(data_list, max_question_length=1018, max_answer_length=1024):\n    tokenized_data_list = []\n    for question, answer in data_list:\n        # Tokenize and convert to indices\n        question_tokens = tokenizer.encode(question, add_special_tokens=True)\n        answer_tokens = tokenizer.encode(answer, add_special_tokens=True)\n\n        # Pad sequences to specified lengths\n        padded_question_tokens = torch.tensor(question_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_question_length - len(question_tokens)))\n        padded_answer_tokens = torch.tensor(answer_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_answer_length - len(answer_tokens)))\n\n        # Append to the tokenized_data_list only if both token lists are not empty\n        if len(question_tokens) > 0 and len(answer_tokens) > 0:\n            tokenized_data_list.append((padded_question_tokens, padded_answer_tokens))\n\n    return tokenized_data_list","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:20.774754Z","iopub.execute_input":"2024-06-11T12:55:20.775144Z","iopub.status.idle":"2024-06-11T12:55:20.792640Z","shell.execute_reply.started":"2024-06-11T12:55:20.775118Z","shell.execute_reply":"2024-06-11T12:55:20.791629Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Lists to store scores\ntrain_bleu_scores = []\ntrain_bert_scores = []\ntrain_rouge1_scores = []\ntrain_rouge2_scores = []\ntrain_rougeL_scores = []\n\nval_bleu_scores = []\nval_bert_scores = []\nval_rouge1_scores = []\nval_rouge2_scores = []\nval_rougeL_scores = []","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:28.820262Z","iopub.execute_input":"2024-06-11T12:55:28.820636Z","iopub.status.idle":"2024-06-11T12:55:28.825754Z","shell.execute_reply.started":"2024-06-11T12:55:28.820610Z","shell.execute_reply":"2024-06-11T12:55:28.824834Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train = '/kaggle/input/squad-20/train-v2.0.json'\ndev = '/kaggle/input/squad-20/dev-v2.0.json'\ntrain_df = squad_json_to_dataframe(train)\ndev_df = squad_json_to_dataframe(dev)\n\ntrain_df['question_context'] = 'Question: ' + train_df['question'] + ' Context: ' + train_df['context']\ndev_df['question_context'] = 'Question: ' + dev_df['question'] + ' Context: ' + dev_df['context']\n# Extract 'text' value from 'answers' column for each row\ntrain_df['answers'] = train_df['answers'].apply(lambda x: x[0]['text'] if len(x) > 0 else None)\ndev_df['answers'] = dev_df['answers'].apply(lambda x: x[0]['text'] if len(x) > 0 else None)\n\n\n# Extract 'question_context' column into a list\nquestion_context_list_train = train_df['question_context'].tolist()\n# Extract 'answers' column into a list\nanswers_list_train = train_df['answers'].tolist()\n\n\n# Extract 'question_context' column into a list\nquestion_context_list_dev = dev_df['question_context'].tolist()\n# Extract 'answers' column into a list\nanswers_list_dev = dev_df['answers'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:31.992589Z","iopub.execute_input":"2024-06-11T12:55:31.992979Z","iopub.status.idle":"2024-06-11T12:55:42.970127Z","shell.execute_reply.started":"2024-06-11T12:55:31.992947Z","shell.execute_reply":"2024-06-11T12:55:42.969139Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"question_context_list_train = question_context_list_train[:100]\nanswers_list_train = answers_list_train[:100]\n\nquestion_context_list_dev = question_context_list_dev[:30]\nanswers_list_dev = answers_list_dev[:30]\n\nquestion_context_list_train = preprocess_text_en(question_context_list_train)\nanswers_list_train = preprocess_text_en(answers_list_train)\n\nquestion_context_list_dev = preprocess_text_en(question_context_list_dev)\nanswers_list_dev = preprocess_text_en(answers_list_dev)\n\n\nmapped_list_1 = list(zip(question_context_list_train, answers_list_train))\nmapped_list_2 = list(zip(question_context_list_dev, answers_list_dev))","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:45.807063Z","iopub.execute_input":"2024-06-11T12:55:45.807634Z","iopub.status.idle":"2024-06-11T12:55:46.163182Z","shell.execute_reply.started":"2024-06-11T12:55:45.807602Z","shell.execute_reply":"2024-06-11T12:55:46.162445Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.shape(mapped_list_1))\nprint(mapped_list_1[1][1])\nprint(\"\\n\\n\",mapped_list_1[0][1])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:49.638054Z","iopub.execute_input":"2024-06-11T12:55:49.638408Z","iopub.status.idle":"2024-06-11T12:55:49.644662Z","shell.execute_reply.started":"2024-06-11T12:55:49.638378Z","shell.execute_reply":"2024-06-11T12:55:49.643681Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(100, 2)\nsinging dancing\n\n\n late 1990s\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Define a pad token and add it to the tokenizer\npad_token = tokenizer.eos_token\ntokenizer.add_tokens([pad_token])\n\n\ntokenized_data_train_list = tokenize_and_pad(mapped_list_1, 1018, 1024)\ntokenized_data_val_list = tokenize_and_pad(mapped_list_2, 1018, 1024)\n# question_context_list_train, answers_list_train = zip(*tokenized_data_train_list)\n# question_context_list_val, answers_list_val = zip(*tokenized_data_val_list)\n# print(len(question_context_list_train),len(answers_list_train))","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:52.262283Z","iopub.execute_input":"2024-06-11T12:55:52.262637Z","iopub.status.idle":"2024-06-11T12:55:54.121061Z","shell.execute_reply.started":"2024-06-11T12:55:52.262606Z","shell.execute_reply":"2024-06-11T12:55:54.120266Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39e081acf544ea4a683f922269e5862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2929806623694137aa941c660b6bf3b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e91eea104a4561bd1892947ed72cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066a71bd03664f8daddbcf0d9217e86b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73faf971240483ebfbb0a73fcb6ed47"}},"metadata":{}}]},{"cell_type":"code","source":"print(len(tokenized_data_train_list))\nprint(tokenized_data_train_list[0][0])\nprint(\"\\n\\n\",tokenized_data_train_list[0][1])\n##print(\"\\n\\n\",tokenized_data_train_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:55.279204Z","iopub.execute_input":"2024-06-11T12:55:55.279577Z","iopub.status.idle":"2024-06-11T12:55:55.314560Z","shell.execute_reply.started":"2024-06-11T12:55:55.279548Z","shell.execute_reply":"2024-06-11T12:55:55.313688Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"100\ntensor([25652,  1058,   307,  ..., 50256, 50256, 50256])\n\n\n tensor([17660,  6303,    82,  ..., 50256, 50256, 50256])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize lists to store input and target ids\ninput_ids_train = []\ntarget_ids_train = []\n\n# Specify maximum lengths\nmax_question_length = 1018\nmax_answer_length = 1024\n\n# Iterate through the tokenized data\nfor padded_question_tokens, padded_answer_tokens in tokenized_data_train_list:\n    # Truncate question tokens if greater than max_question_length\n    truncated_question_tokens = padded_question_tokens[:max_question_length]\n\n    # Truncate answer tokens if greater than max_answer_length\n    truncated_answer_tokens = padded_answer_tokens[:max_answer_length]\n\n    # Append truncated question tokens to input_ids_train\n    input_ids_train.append(truncated_question_tokens)\n\n    # Append truncated answer tokens to target_ids_train\n    target_ids_train.append(truncated_answer_tokens)\n\n# Convert the lists to PyTorch tensors\ninput_ids_train = torch.stack(input_ids_train)\ntarget_ids_train = torch.stack(target_ids_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:55:57.611207Z","iopub.execute_input":"2024-06-11T12:55:57.611573Z","iopub.status.idle":"2024-06-11T12:55:57.627002Z","shell.execute_reply.started":"2024-06-11T12:55:57.611543Z","shell.execute_reply":"2024-06-11T12:55:57.625965Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(input_ids_train.shape)\nprint(target_ids_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:56:00.165124Z","iopub.execute_input":"2024-06-11T12:56:00.165496Z","iopub.status.idle":"2024-06-11T12:56:00.170545Z","shell.execute_reply.started":"2024-06-11T12:56:00.165466Z","shell.execute_reply":"2024-06-11T12:56:00.169570Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"torch.Size([100, 1018])\ntorch.Size([100, 1024])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize lists to store input and target ids\ninput_ids_val = []\ntarget_ids_val = []\n\n# Specify maximum lengths\nmax_question_length = 1018\nmax_answer_length = 1024\n\n# Iterate through the tokenized data\nfor padded_question_tokens, padded_answer_tokens in tokenized_data_val_list:\n    # Truncate question tokens if greater than max_question_length\n    truncated_question_tokens = padded_question_tokens[:max_question_length]\n\n    # Truncate answer tokens if greater than max_answer_length\n    truncated_answer_tokens = padded_answer_tokens[:max_answer_length]\n\n    # Append truncated question tokens to input_ids_train\n    input_ids_val.append(truncated_question_tokens)\n\n    # Append truncated answer tokens to target_ids_train\n    target_ids_val.append(truncated_answer_tokens)\n\n# Convert the lists to PyTorch tensors\ninput_ids_val = torch.stack(input_ids_val)\ntarget_ids_val = torch.stack(target_ids_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:56:03.297662Z","iopub.execute_input":"2024-06-11T12:56:03.298036Z","iopub.status.idle":"2024-06-11T12:56:03.305262Z","shell.execute_reply.started":"2024-06-11T12:56:03.298006Z","shell.execute_reply":"2024-06-11T12:56:03.304412Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ngpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Define the number of prompts and embedding size\nnum_prompts = 6  # \"summarize the following text\"\nembedding_size = 768\n\n# Define a specific sentence\nsentence = \"Answer the question using given context\"\n\n# Tokenize the sentence\ninput_ids = tokenizer.encode(sentence, return_tensors='pt')\n\n# Get the embeddings for the input_ids from the GPT-2 model\ngpt2_embeddings = gpt2_model.transformer.wte(input_ids)\n\n# Create an embedding layer for soft prompts and initialize with the sentence embeddings\nsoft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\nsoft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))\n\n# Concatenate soft prompt embeddings at the beginning of the input sequence\nclass GPT2WithPromptTuning(nn.Module):\n    def __init__(self, gpt2_model, soft_prompt_embeddings):\n        super(GPT2WithPromptTuning, self).__init__()\n        self.gpt2_model = gpt2_model\n        self.soft_prompt_embeddings = soft_prompt_embeddings\n    \n    def forward(self, input_ids, soft_prompt_ids):\n        # Get the embeddings for the input_ids from the GPT-2 model\n        gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)\n        # Get the embeddings for the soft prompts\n        soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)\n        # print(gpt2_embeddings.shape,soft_prompt_embeds.shape )\n#         print(\"gpt2_embeddings\",gpt2_embeddings.shape,\"soft_prompt_embeds\",soft_prompt_embeds.shape)\n        # Concatenate the embeddings\n        embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)\n        \n        # Pass the concatenated embeddings through the GPT-2 model\n        outputs = self.gpt2_model(inputs_embeds=embeddings)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:57:48.139080Z","iopub.execute_input":"2024-06-11T12:57:48.139820Z","iopub.status.idle":"2024-06-11T12:57:51.576677Z","shell.execute_reply.started":"2024-06-11T12:57:48.139760Z","shell.execute_reply":"2024-06-11T12:57:51.575757Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3591033b1a3d4ff5aedbf1b57388cc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7cff720fc14ed49194b261f7945bda"}},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the model\nmodel = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\n\n# Freeze GPT-2 model weights\nfor param in model.gpt2_model.parameters():\n    param.requires_grad = False\n\n# Define hyperparameters\nbatch_size = 4\nepochs = 2\nlearning_rate = 2e-3\ngradient_clip_value = 1.0\n\n# Define optimizer and criterion\noptimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\nsoft_prompt_ids = torch.tensor([0, 1, 2, 3 ,4 ,5]).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T12:58:08.171168Z","iopub.execute_input":"2024-06-11T12:58:08.172165Z","iopub.status.idle":"2024-06-11T12:58:08.404064Z","shell.execute_reply.started":"2024-06-11T12:58:08.172125Z","shell.execute_reply":"2024-06-11T12:58:08.403057Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nfrom transformers import BertTokenizer, BertForMaskedLM, BertModel\nfrom bert_score import BERTScorer\n\ndevice = \"cuda\"\n# Move model to GPU\nmodel.to(device)\n\n\n# Training loop\nfor epoch in range(epochs):\n    # Create a tqdm progress bar for the training data\n    data_iterator = tqdm(zip(input_ids_train, target_ids_train), desc=f'Epoch {epoch + 1}', total=len(input_ids_train))\n    \n    for input_ids, target_ids in data_iterator:\n        optimizer.zero_grad()\n\n        # Move input and target tensors to GPU\n        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n        \n        # Assuming you have a soft_prompt_ids for each training instance\n        # If not, you might need to modify this part accordingly\n        outputs = model(input_ids, soft_prompt_ids.to(device))\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n\n        loss = criterion(logits, target_ids)\n        loss.backward()\n\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n\n        optimizer.step()\n\n        # Update the progress bar description with the current loss\n        data_iterator.set_postfix(loss=loss.item())\n\n        # Convert tensor predictions and references to lists\n        predictions = logits.argmax(dim=-1).squeeze(0).tolist()\n        references = target_ids.squeeze(0).tolist()\n\n        # Calculate BLEU Score for training\n        bleu_score = calculate_bleu_score([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n        train_bleu_scores.append(bleu_score)\n\n        # Calculate BERTScore for training\n        bert_precision, bert_recall, bert_f1 = calculate_bert_score([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n        train_bert_scores.append(bert_f1)\n\n        # Calculate ROUGE Scores for training\n        rouge1, rouge2, rougeL = calculate_rouge_scores([tokenizer.decode(predictions)], [tokenizer.decode(references)])\n        train_rouge1_scores.append(rouge1)\n        train_rouge2_scores.append(rouge2)\n        train_rougeL_scores.append(rougeL)\n\n    # Validation loop\n    model.eval()\n    val_losses = []\n    val_bleu_scores_epoch = []\n    val_bert_scores_epoch = []\n    val_rouge1_scores_epoch = []\n    val_rouge2_scores_epoch = []\n    val_rougeL_scores_epoch = []\n    with torch.no_grad():\n        for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):\n            input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)\n            outputs_val = model(input_ids_val, soft_prompt_ids.to(device))\n            logits_val = outputs_val.logits if hasattr(outputs_val, \"logits\") else outputs_val.last_hidden_state\n            loss_val = criterion(logits_val, target_ids_val)\n            val_losses.append(loss_val.item())\n\n            # Convert tensor predictions and references to lists\n            predictions_val = logits_val.argmax(dim=-1).squeeze(0).tolist()\n            references_val = target_ids_val.squeeze(0).tolist()\n\n            # Calculate BLEU Score for validation\n            bleu_score_val = calculate_bleu_score([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n            \n            val_bleu_scores_epoch.append(bleu_score_val)\n\n            # Calculate BERTScore for validation\n            bert_precision_val, bert_recall_val, bert_f1_val = calculate_bert_score([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n            val_bert_scores_epoch.append(bert_f1_val)\n\n            # Calculate ROUGE Scores for validation\n            rouge1_val, rouge2_val, rougeL_val = calculate_rouge_scores([tokenizer.decode(predictions_val)], [tokenizer.decode(references_val)])\n            val_rouge1_scores_epoch.append(rouge1_val)\n            val_rouge2_scores_epoch.append(rouge2_val)\n            val_rougeL_scores_epoch.append(rougeL_val)\n\n    # Calculate average validation loss\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    print(\"epoch :\", epoch + 1,\"train_loss :\", loss.item(),\"val_loss :\", avg_val_loss)\n\n    # Calculate average validation scores\n    avg_bleu_score_val = sum(val_bleu_scores_epoch) / len(val_bleu_scores_epoch)\n    avg_bert_score_val = sum(val_bert_scores_epoch) / len(val_bert_scores_epoch)\n    avg_rouge1_score_val = sum(val_rouge1_scores_epoch) / len(val_rouge1_scores_epoch)\n    avg_rouge2_score_val = sum(val_rouge2_scores_epoch) / len(val_rouge2_scores_epoch)\n    avg_rougeL_score_val = sum(val_rougeL_scores_epoch) / len(val_rougeL_scores_epoch)\n\n    print(\"Validation BLEU Score:\", avg_bleu_score_val)\n    print(\"Validation BERTScore:\", avg_bert_score_val)\n    print(\"Validation ROUGE-1 Score:\", avg_rouge1_score_val)\n    print(\"Validation ROUGE-2 Score:\", avg_rouge2_score_val)\n    print(\"Validation ROUGE-L Score:\", avg_rougeL_score_val)\n\n    # Append validation scores\n    val_bleu_scores.append(avg_bleu_score_val)\n    val_bert_scores.append(avg_bert_score_val)\n    val_rouge1_scores.append(avg_rouge1_score_val)\n    val_rouge2_scores.append(avg_rouge2_score_val)\n    val_rougeL_scores.append(avg_rougeL_score_val)\n\n    # Set the model back to training mode\n    model.train()\n\n# Close the tqdm progress bar\ndata_iterator.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:00:57.771367Z","iopub.execute_input":"2024-06-11T13:00:57.772384Z","iopub.status.idle":"2024-06-11T13:06:10.432480Z","shell.execute_reply.started":"2024-06-11T13:00:57.772346Z","shell.execute_reply":"2024-06-11T13:06:10.431072Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Epoch 1:   0%|          | 0/100 [00:01<?, ?it/s, loss=12.2]2024-06-11 13:01:00.601320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-11 13:01:00.601445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-11 13:01:00.726488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7bc876e277a452da9a45cc885d2640a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6307c4346f9e4a18a21a91c677eb90be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb05ee345db4b44b469f015ece22947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b49d81f96e04812a6fb5a22ddbe569f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc610f2faac4728b90b0e89691bd9b3"}},"metadata":{}},{"name":"stderr","text":"Epoch 1:  57%|█████▋    | 57/100 [01:24<01:25,  1.98s/it, loss=4.45]/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\nEpoch 1: 100%|██████████| 100/100 [02:25<00:00,  1.46s/it, loss=0.0877]\n","output_type":"stream"},{"name":"stdout","text":"epoch : 1 train_loss : 0.08770941942930222 val_loss : 0.08353108167648315\nValidation BLEU Score: 0.0\nValidation BERTScore: tensor(0.9858)\nValidation ROUGE-1 Score: 0.9959803621532478\nValidation ROUGE-2 Score: 0.9958364711772846\nValidation ROUGE-L Score: 0.9959803621532478\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 100/100 [02:24<00:00,  1.45s/it, loss=0.455]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids_val, target_ids_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_ids_val, target_ids_val):\n\u001b[1;32m     67\u001b[0m     input_ids_val, target_ids_val \u001b[38;5;241m=\u001b[39m input_ids_val\u001b[38;5;241m.\u001b[39mto(device), target_ids_val\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 68\u001b[0m     outputs_val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     logits_val \u001b[38;5;241m=\u001b[39m outputs_val\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs_val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs_val\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     70\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m criterion(logits_val, target_ids_val)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[18], line 38\u001b[0m, in \u001b[0;36mGPT2WithPromptTuning.forward\u001b[0;34m(self, input_ids, soft_prompt_ids)\u001b[0m\n\u001b[1;32m     34\u001b[0m         soft_prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_prompt_embeddings(soft_prompt_ids)\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# print(gpt2_embeddings.shape,soft_prompt_embeds.shape )\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#         print(\"gpt2_embeddings\",gpt2_embeddings.shape,\"soft_prompt_embeds\",soft_prompt_embeds.shape)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# Concatenate the embeddings\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msoft_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_embeddings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# Pass the concatenated embeddings through the GPT-2 model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_model(inputs_embeds\u001b[38;5;241m=\u001b[39membeddings)\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"],"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 2 and 1","output_type":"error"}]},{"cell_type":"code","source":"# Calculate average scores for training\navg_train_bleu_score = sum(train_bleu_scores) / len(train_bleu_scores)\navg_train_bert_score = sum(train_bert_scores) / len(train_bert_scores)\navg_train_rouge1_score = sum(train_rouge1_scores) / len(train_rouge1_scores)\navg_train_rouge2_score = sum(train_rouge2_scores) / len(train_rouge2_scores)\navg_train_rougeL_score = sum(train_rougeL_scores) / len(train_rougeL_scores)\n\nprint(\"Average Training BLEU Score:\", avg_train_bleu_score)\nprint(\"Average Training BERTScore:\", avg_train_bert_score)\nprint(\"Average Training ROUGE-1 Score:\", avg_train_rouge1_score)\nprint(\"Average Training ROUGE-2 Score:\", avg_train_rouge2_score)\nprint(\"Average Training ROUGE-L Score:\", avg_train_rougeL_score)\n\n# Calculate average scores for validation\navg_val_bleu_score = sum(val_bleu_scores) / len(val_bleu_scores)\navg_val_bert_score = sum(val_bert_scores) / len(val_bert_scores)\navg_val_rouge1_score = sum(val_rouge1_scores) / len(val_rouge1_scores)\navg_val_rouge2_score = sum(val_rouge2_scores) / len(val_rouge2_scores)\navg_val_rougeL_score = sum(val_rougeL_scores) / len(val_rougeL_scores)\n\nprint(\"Average Validation BLEU Score:\", avg_val_bleu_score)\nprint(\"Average Validation BERTScore:\", avg_val_bert_score)\nprint(\"Average Validation ROUGE-1 Score:\", avg_val_rouge1_score)\nprint(\"Average Validation ROUGE-2 Score:\", avg_val_rouge2_score)\nprint(\"Average Validation ROUGE-L Score:\", avg_val_rougeL_score)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:06:24.225696Z","iopub.execute_input":"2024-06-11T13:06:24.226382Z","iopub.status.idle":"2024-06-11T13:06:24.237977Z","shell.execute_reply.started":"2024-06-11T13:06:24.226352Z","shell.execute_reply":"2024-06-11T13:06:24.237060Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Average Training BLEU Score: 0.0017047553984649772\nAverage Training BERTScore: tensor(0.7555)\nAverage Training ROUGE-1 Score: 0.4488093882145418\nAverage Training ROUGE-2 Score: 0.43119072490326354\nAverage Training ROUGE-L Score: 0.4487952588043352\nAverage Validation BLEU Score: 0.0\nAverage Validation BERTScore: tensor(0.9858)\nAverage Validation ROUGE-1 Score: 0.9959803621532478\nAverage Validation ROUGE-2 Score: 0.9958364711772846\nAverage Validation ROUGE-L Score: 0.9959803621532478\n","output_type":"stream"}]},{"cell_type":"code","source":" # Save model weights\ntorch.save(model.state_dict(), 'QNA.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:06:33.798574Z","iopub.execute_input":"2024-06-11T13:06:33.798956Z","iopub.status.idle":"2024-06-11T13:06:34.424800Z","shell.execute_reply.started":"2024-06-11T13:06:33.798924Z","shell.execute_reply":"2024-06-11T13:06:34.423940Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Load model architecture\nmodel = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\nprint(\"Before \",model)\n# Load the saved model weights\nmodel.load_state_dict(torch.load('QNA.pth'))\nprint(\"After \",model)\n\n# Move model to the desired device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:06:50.101251Z","iopub.execute_input":"2024-06-11T13:06:50.101625Z","iopub.status.idle":"2024-06-11T13:06:50.600109Z","shell.execute_reply.started":"2024-06-11T13:06:50.101597Z","shell.execute_reply":"2024-06-11T13:06:50.599178Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Before  GPT2WithPromptTuning(\n  (gpt2_model): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (soft_prompt_embeddings): Embedding(6, 768)\n)\nAfter  GPT2WithPromptTuning(\n  (gpt2_model): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (soft_prompt_embeddings): Embedding(6, 768)\n)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"GPT2WithPromptTuning(\n  (gpt2_model): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (soft_prompt_embeddings): Embedding(6, 768)\n)"},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"Question: [Which NFL team represented the NFC at Super Bowl 50?] Context: [Carolina Panthers]\"\ninput_ids = tokenizer.encode(input_text, return_tensors='tf')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:08:55.549255Z","iopub.execute_input":"2024-06-11T13:08:55.550080Z","iopub.status.idle":"2024-06-11T13:08:55.555557Z","shell.execute_reply.started":"2024-06-11T13:08:55.550044Z","shell.execute_reply":"2024-06-11T13:08:55.554600Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"input_ids.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:08:57.651645Z","iopub.execute_input":"2024-06-11T13:08:57.652411Z","iopub.status.idle":"2024-06-11T13:08:57.658374Z","shell.execute_reply.started":"2024-06-11T13:08:57.652377Z","shell.execute_reply":"2024-06-11T13:08:57.657390Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TensorShape([1, 21])"},"metadata":{}}]},{"cell_type":"code","source":"# Move soft_prompt_embeddings tensor to the same device as gpt2_embeddings\nsoft_prompt_embeds = soft_prompt_embeddings(soft_prompt_ids)\n# Unsqueeze to make it 3D\n\nprint(soft_prompt_embeds.shape)\n# Concatenate the embeddings\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:11:32.880244Z","iopub.execute_input":"2024-06-11T13:11:32.881018Z","iopub.status.idle":"2024-06-11T13:11:32.886721Z","shell.execute_reply.started":"2024-06-11T13:11:32.880979Z","shell.execute_reply":"2024-06-11T13:11:32.885741Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"torch.Size([6, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"soft_prompt_ids = torch.tensor([0, 1, 2, 3, 4, 5]).to(device)\nsoft_prompt_ids.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:07:03.695421Z","iopub.execute_input":"2024-06-10T17:07:03.695761Z","iopub.status.idle":"2024-06-10T17:07:03.702615Z","shell.execute_reply.started":"2024-06-10T17:07:03.695737Z","shell.execute_reply":"2024-06-10T17:07:03.701643Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([6])"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:42:31.659443Z","iopub.execute_input":"2024-06-10T17:42:31.660067Z","iopub.status.idle":"2024-06-10T17:42:31.663995Z","shell.execute_reply.started":"2024-06-10T17:42:31.660034Z","shell.execute_reply":"2024-06-10T17:42:31.662997Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:44:40.921723Z","iopub.execute_input":"2024-06-10T17:44:40.922065Z","iopub.status.idle":"2024-06-10T17:44:40.968567Z","shell.execute_reply.started":"2024-06-10T17:44:40.922038Z","shell.execute_reply":"2024-06-10T17:44:40.967278Z"},"trusted":true},"execution_count":54,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nput_ids_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(input_ids\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 2\u001b[0m soft_prompt_ids_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[43msoft_prompt_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Then pass them to your TensorFlow model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids_tf, soft_prompt_ids_tf)\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."],"ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:15:51.841946Z","iopub.execute_input":"2024-06-11T13:15:51.842867Z","iopub.status.idle":"2024-06-11T13:15:51.977364Z","shell.execute_reply.started":"2024-06-11T13:15:51.842826Z","shell.execute_reply":"2024-06-11T13:15:51.976093Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Get predicted token IDs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[36], line 21\u001b[0m, in \u001b[0;36mGPT2WithPromptTuning.forward\u001b[0;34m(self, input_ids, soft_prompt_ids)\u001b[0m\n\u001b[1;32m     18\u001b[0m     soft_prompt_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(soft_prompt_ids)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get the embeddings for the input_ids from the GPT-2 model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m gpt2_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get the embeddings for the soft prompts\u001b[39;00m\n\u001b[1;32m     24\u001b[0m soft_prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_prompt_embeddings(soft_prompt_ids)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'GPT2WithPromptTuning' object has no attribute 'transformer'"],"ename":"AttributeError","evalue":"'GPT2WithPromptTuning' object has no attribute 'transformer'","output_type":"error"}]},{"cell_type":"code","source":"\n# Forward pass\nwith torch.no_grad():\n    outputs = model(input_ids,soft_prompt_ids)\n    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n\n# Get predicted token IDs\npredicted_token_ids = torch.argmax(logits, dim=None).squeeze()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T13:12:26.575673Z","iopub.execute_input":"2024-06-11T13:12:26.576328Z","iopub.status.idle":"2024-06-11T13:12:27.053710Z","shell.execute_reply.started":"2024-06-11T13:12:26.576298Z","shell.execute_reply":"2024-06-11T13:12:27.052365Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43msoft_prompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get predicted token IDs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mGPT2WithPromptTuning.forward\u001b[0;34m(self, input_ids, soft_prompt_ids)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, soft_prompt_ids):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Get the embeddings for the input_ids from the GPT-2 model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     gpt2_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Get the embeddings for the soft prompts\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     soft_prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_prompt_embeddings(soft_prompt_ids)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not tensorflow.python.framework.ops.EagerTensor"],"ename":"TypeError","evalue":"embedding(): argument 'indices' (position 2) must be Tensor, not tensorflow.python.framework.ops.EagerTensor","output_type":"error"}]},{"cell_type":"code","source":"gpt2_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:52:28.313879Z","iopub.execute_input":"2024-06-10T17:52:28.314251Z","iopub.status.idle":"2024-06-10T17:52:28.320278Z","shell.execute_reply.started":"2024-06-10T17:52:28.314220Z","shell.execute_reply":"2024-06-10T17:52:28.319347Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 6, 768])"},"metadata":{}}]},{"cell_type":"code","source":"soft_prompt_embeds.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:52:31.147406Z","iopub.execute_input":"2024-06-10T17:52:31.147740Z","iopub.status.idle":"2024-06-10T17:52:31.153238Z","shell.execute_reply.started":"2024-06-10T17:52:31.147714Z","shell.execute_reply":"2024-06-10T17:52:31.152379Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 6, 768])"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-06-10T16:35:54.541156Z","iopub.execute_input":"2024-06-10T16:35:54.542175Z","iopub.status.idle":"2024-06-10T16:35:54.549227Z","shell.execute_reply.started":"2024-06-10T16:35:54.542138Z","shell.execute_reply":"2024-06-10T16:35:54.548152Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"GPT2WithPromptTuning(\n  (gpt2_model): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (soft_prompt_embeddings): Embedding(6, 768)\n)"},"metadata":{}}]},{"cell_type":"code","source":"foundational_outputs_prompt = get_outputs(model, input_ids, max_new_tokens=100)\n\nprint(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:32:15.661061Z","iopub.execute_input":"2024-06-10T15:32:15.661784Z","iopub.status.idle":"2024-06-10T15:32:15.705829Z","shell.execute_reply.started":"2024-06-10T15:32:15.661751Z","shell.execute_reply":"2024-06-10T15:32:15.704533Z"},"trusted":true},"execution_count":91,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m foundational_outputs_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mget_outputs\u001b[49m(model, input_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(foundational_outputs_prompt, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n","\u001b[0;31mNameError\u001b[0m: name 'get_outputs' is not defined"],"ename":"NameError","evalue":"name 'get_outputs' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:29:59.919398Z","iopub.execute_input":"2024-06-06T18:29:59.919783Z","iopub.status.idle":"2024-06-06T18:30:00.021527Z","shell.execute_reply.started":"2024-06-06T18:29:59.919752Z","shell.execute_reply":"2024-06-06T18:30:00.020598Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:59:36.772669Z","iopub.execute_input":"2024-06-06T18:59:36.773400Z","iopub.status.idle":"2024-06-06T18:59:36.900432Z","shell.execute_reply.started":"2024-06-06T18:59:36.773366Z","shell.execute_reply":"2024-06-06T18:59:36.899022Z"},"trusted":true},"execution_count":76,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:916\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    907\u001b[0m         model,\n\u001b[1;32m    908\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    918\u001b[0m load_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model_config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING \u001b[38;5;129;01mor\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mtokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'GPT2WithPromptTuning' object has no attribute 'config'"],"ename":"AttributeError","evalue":"'GPT2WithPromptTuning' object has no attribute 'config'","output_type":"error"}]},{"cell_type":"code","source":"q= \"question:Which NFL team won Super Bowl 50?\"\na = \"answer : Denver Broncos\"\nprompt = f\"Based on the answer to this question, respond with a complete phrase and no explanation {q} {a}\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200,temperature = 0)\nresult = pipe(prompt)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:59:46.948670Z","iopub.execute_input":"2024-06-06T18:59:46.949354Z","iopub.status.idle":"2024-06-06T18:59:47.079474Z","shell.execute_reply.started":"2024-06-06T18:59:46.949320Z","shell.execute_reply":"2024-06-06T18:59:47.078063Z"},"trusted":true},"execution_count":78,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer : Denver Broncos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the answer to this question, respond with a complete phrase and no explanation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(prompt)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:916\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    907\u001b[0m         model,\n\u001b[1;32m    908\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    918\u001b[0m load_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model_config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING \u001b[38;5;129;01mor\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mtokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'GPT2WithPromptTuning' object has no attribute 'config'"],"ename":"AttributeError","evalue":"'GPT2WithPromptTuning' object has no attribute 'config'","output_type":"error"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\nprint(output.decode)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}